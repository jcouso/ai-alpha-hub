# AI Alpha Report ‚Äî February 20, 2026

*Daily intelligence for technical founders who ship*

---

## TL;DR

- üî• **ggml.ai (llama.cpp) joins Hugging Face** ‚Äî Georgi Gerganov and the founding team formally join HF to ensure long-term sustainability of local AI inference. llama.cpp remains open-source and community-driven. HN #1 with 428+ points.
- üö® **Google drops Gemini 3.1 Pro** ‚Äî 77.1% ARC-AGI-2 (2x+ over 3 Pro), 80.6% SWE-Bench, beats Opus 4.6 and GPT-5.2 on most benchmarks. Available NOW in Gemini API, CLI, Antigravity, NotebookLM, and GitHub Copilot. $2/$12 per 1M tokens ‚Äî 7.5x cheaper than Opus on input.
- üèóÔ∏è **Stripe publishes Minions Part 2** ‚Äî 1,300+ PRs merged per week from fully autonomous coding agents. Zero human in the loop until review. Detailed architecture breakdown.
- ‚ö° **Together AI: Consistency Diffusion LMs** ‚Äî Up to 14x faster inference, no quality loss. Potential paradigm shift from autoregressive decoding. HN #5 with 139 points.
- üî¨ **Anthropic publishes agent autonomy study** ‚Äî Claude Code autonomous sessions nearly doubled (25‚Üí45+ min) in 3 months. 40%+ experienced users run full auto-approve.

---

## üß≠ Narrative Radar

### What's Shifting This Week

**1. Open-Source AI Consolidates Around Hugging Face**
ggml.ai ‚Äî the company behind llama.cpp, the foundational library for local AI inference ‚Äî formally joining Hugging Face is a landmark moment. llama.cpp is arguably the single most important open-source project for running models on consumer hardware. This isn't an acquisition that kills a project; it's a sustainability play. HF is providing long-term resources while the ggml team maintains full autonomy over technical decisions. Combined with HF's transformers library, this creates a vertically integrated open-source stack from model definition to local inference. The message: the open-source AI infrastructure layer is consolidating, and Hugging Face is becoming its home.

**2. The Benchmark Race Just Flipped ‚Äî Google Retakes the Crown**
Gemini 3.1 Pro landing with 77.1% on ARC-AGI-2 (more than double Gemini 3 Pro), 44.4% on Humanity's Last Exam (vs Opus 4.6's 40%, GPT-5.2's 34.5%), and 80.6% SWE-Bench Verified isn't just another model bump ‚Äî it's Google genuinely competing at the frontier again. The pricing ($2/$12 per 1M tokens, 7.5x cheaper than Opus on input) makes this especially disruptive. With Gemini CLI, Antigravity IDE, and now GitHub Copilot integration, Google is competing across the entire developer surface for the first time. The question: does benchmark lead translate to practitioner adoption?

**3. Coding Agents Are Graduating From "Experiment" to "Infrastructure"**
Stripe publishing detailed numbers (1,300+ PRs/week from autonomous agents, zero human until review) is the clearest signal yet that coding agents aren't a novelty ‚Äî they're production infrastructure at top-tier engineering orgs. Simultaneously, Anthropic publishing data showing Claude Code sessions nearly doubling in autonomous runtime and 40%+ of experienced users running full auto-approve means the human-in-the-loop is shrinking fast. The agent autonomy curve is steepening.

### What's Fading
- The "Anthropic OAuth lockdown" panic from this week is settling ‚Äî people have switched or adapted
- Pentagon vs. Anthropic is becoming background noise without new material developments
- India AI Summit generated lots of announcements but no breakthrough products

---

## üöÄ Top Early Moves

### 1. ggml.ai (llama.cpp) Joins Hugging Face üî•üî•üî•
**Score: 9.5/10** | CONFIRMED | Open-source infrastructure, long-term implications

Georgi Gerganov and the founding ggml.ai team have formally joined Hugging Face. llama.cpp ‚Äî the library that made running LLMs on consumer hardware possible ‚Äî now has long-term institutional backing. Key points:

- **llama.cpp remains 100% open-source and community-driven** ‚Äî no license changes, no direction changes
- ggml team continues full-time on llama.cpp with complete technical autonomy
- Joint focus: seamless transformers ‚Üî ggml integration (single-click model conversion), better packaging, and making local inference "ubiquitous and readily available everywhere"
- HF engineers have already been contributing core features: multi-modal support, inference server, GGUF format improvements
- Vision: "open-source superintelligence accessible to the world"

**Why this matters beyond the headline:** This is the infrastructure play. HF already owns model distribution (Hub) and model definition (transformers). Now they own the inference layer (llama.cpp/ggml). That's the full open-source AI stack under one roof. For practitioners: expect faster model support after releases, better tooling, and a more cohesive local AI experience.

HN #1 with 260+ points and climbing. Community reception is overwhelmingly positive.

**Source:** [GitHub Discussion](https://github.com/ggml-org/llama.cpp/discussions/19759) | [HN](https://news.ycombinator.com/item?id=47088037)

---

### 2. Google Releases Gemini 3.1 Pro ‚Äî Benchmark Leader at Fraction of the Cost üî•üî•üî•
**Score: 9.5/10** | CONFIRMED | Major new model, test it today

Google dropped Gemini 3.1 Pro on Feb 19, building on the "upgraded core intelligence" that powered last week's Gemini 3 Deep Think update. The benchmarks are genuinely impressive:

| Benchmark | Gemini 3.1 Pro | Claude Opus 4.6 | GPT-5.2 |
|-----------|---------------|-----------------|---------|
| ARC-AGI-2 | **77.1%** | ‚Äî | ‚Äî |
| HLE | **44.4%** | 40.0% | 34.5% |
| GPQA Diamond | **94.3%** | ‚Äî | ‚Äî |
| SWE-Bench Verified | **80.6%** | ‚Äî | ‚Äî |
| LiveCodeBench Pro Elo | **2887** | ‚Äî | ‚Äî |
| Terminal-Bench 2.0 | **68.5%** | ‚Äî | ‚Äî |

**Pricing:** $2 input / $12 output per 1M tokens ‚Äî **7.5x cheaper than Opus 4.6 on input**. 1M token context window.

**Where to try it:**
- [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-3.1-pro-preview) (preview)
- [Gemini CLI](https://geminicli.com/)
- [Google Antigravity IDE](https://antigravity.google/)
- Vertex AI, NotebookLM (rolling out)
- GitHub Copilot (public preview, see below)

**JetBrains quote:** "Up to 15% improvement over best Gemini 3 Pro Preview runs. Stronger, faster, more efficient, requiring fewer output tokens."

HN #1 on Feb 19 with 880 points and 861 comments.

**Source:** [blog.google](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/) | [Google Cloud blog](https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-1-pro-on-gemini-cli-gemini-enterprise-and-vertex-ai) | [Ars Technica](https://arstechnica.com/google/2026/02/google-announces-gemini-3-1-pro-says-its-better-at-complex-problem-solving/) | [VentureBeat](https://venturebeat.com/technology/google-launches-gemini-3-1-pro-retaking-ai-crown-with-2x-reasoning)

---

### 3. Stripe "Minions" Part 2 ‚Äî 1,300+ PRs/Week From Autonomous Agents üèóÔ∏è
**Score: 9.0/10** | CONFIRMED | The best public data on coding agents at scale

Stripe published Part 2 of their Minions blog series. Minions are Stripe's fully unattended coding agents ‚Äî given a task, they write code, run tests, fix issues, and submit a pull request with zero human in the loop until review. Key numbers:

- **1,300+ PRs merged per week** from Minions alone
- Tasks range from fixing flaky tests to implementing features
- "One-shot, end-to-end" ‚Äî they don't iterate with a human mid-task

This is the most detailed public account of autonomous coding agents operating at scale in a top-tier engineering org. Required reading for anyone building or evaluating coding agent workflows.

**Source:** [stripe.dev/blog](https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents-part-2) | [HN](https://news.ycombinator.com/item?id=47086557)

---

### 4. GitHub Copilot: Gemini 3.1 Pro + Model Picker for Enterprise + Zed GA + Model Purge
**Score: 8.5/10** | CONFIRMED | Four changelog updates, all actionable

Four GitHub Changelog entries dropped Feb 19:

**a) Gemini 3.1 Pro in Copilot (public preview)**
Available in VS Code (chat, ask, edit, agent), Visual Studio, github.com, GitHub Mobile. Pro, Pro+, Business, Enterprise. Excels at "edit-then-test loops with high tool precision."

**b) Model picker for Copilot coding agent ‚Üí Business/Enterprise**
Previously Pro/Pro+ only. Now Business and Enterprise users can select from: Claude Opus 4.5/4.6, Sonnet 4.5/4.6, GPT-5.1-Codex-Max, GPT-5.2-Codex, GPT-5.3-Codex. Default (Auto): Claude Sonnet 4.6.

**c) GitHub Copilot in Zed ‚Äî Generally Available**
GitHub now officially supports using Copilot subscriptions (Pro, Pro+, Business, Enterprise) with the Zed editor. No additional license needed. Zed ‚Äî the high-performance Rust-based editor from the creators of Atom and Tree-sitter ‚Äî now has first-class Copilot integration.

**d) Deprecated models: Claude Opus 4.1, GPT-5, GPT-5-Codex**
All deprecated Feb 17 across all Copilot experiences. Suggested alternatives: Opus 4.6, GPT-5.2, GPT-5.2-Codex.

**Source:** [Gemini 3.1 Pro in Copilot](https://github.blog/changelog/2026-02-19-gemini-3-1-pro-is-now-in-public-preview-in-github-copilot/) | [Model picker](https://github.blog/changelog/2026-02-19-model-picker-for-copilot-coding-agent-for-copilot-business-and-enterprise-users/) | [Zed GA](https://github.blog/changelog/2026-02-19-github-copilot-support-in-zed-generally-available/) | [Deprecations](https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated/)

---

### 5. Anthropic Publishes Agent Autonomy Study üî¨
**Score: 8.0/10** | CONFIRMED | Hard data on how coding agents are actually used

Anthropic published "Measuring AI agent autonomy in practice" ‚Äî analyzing millions of Claude Code interactions using their privacy-preserving tool Clio. Key findings:

- **Autonomous session length nearly doubled** from <25 min to >45 min in 3 months (smooth across model releases, suggesting users are granting more autonomy, not just capability gains)
- **40%+ experienced users use full auto-approve** (vs ~20% new users)
- **Claude Code pauses for clarification more often than humans interrupt it** ‚Äî agent-initiated stops > human interrupts on complex tasks
- **Most usage is coding**, but data shows the next agent targets: research, writing, data analysis
- METR's 50th-percentile metric shows agents now doing ~5 hours of human-equivalent work

This is the first large-scale empirical study of real-world agent autonomy patterns. The trend line is clear: autonomy is expanding rapidly.

**Source:** [anthropic.com/research/measuring-agent-autonomy](https://www.anthropic.com/research/measuring-agent-autonomy) | [Latent Space analysis](https://www.latent.space/p/ainews-anthropics-agent-autonomy)

---

### 6. Together AI: Consistency Diffusion Language Models ‚Äî 14x Faster Inference ‚ö°
**Score: 8.0/10** | CONFIRMED | Potential paradigm shift in LLM inference

Together AI published a paper on Consistency Diffusion Language Models (CDLM) ‚Äî a post-training recipe that makes diffusion-based language models viable for production inference. Instead of autoregressive token-by-token generation, CDLMs iteratively refine masked sequences, generating multiple tokens per step.

**Key results:** Up to 14x faster inference vs autoregressive models with no quality degradation. Uses block-wise causal masking for exact KV caching compatibility (solving the main bottleneck of diffusion LMs). The approach works as a post-training step on existing models.

**Why this matters:** If diffusion LMs become practical, the entire inference stack changes. 14x throughput improvement means agent loops that take minutes could take seconds. HN #5 with 139 points and 47 comments ‚Äî the technical community is paying attention.

**Source:** [together.ai/blog](https://www.together.ai/blog/consistency-diffusion-language-models) | [HN discussion](https://news.ycombinator.com/item?id=47083648)

---

### 7. Taalas: Custom Silicon Llama at 17K Tokens/Sec üßÆ
**Score: 7.5/10** | CONFIRMED | Hardware inference breakthrough, HN #1

Taalas unveiled their first product: a hard-wired Llama 3.1 8B implemented in custom silicon, achieving 17K tokens/sec per user ‚Äî nearly 10x faster than current state of the art. The startup converts any AI model into custom silicon in ~2 months.

**The thesis:** Instead of scaling data centers, specialize hardware for individual models. Like how ENIAC ‚Üí transistors ‚Üí smartphones happened for computing, AI inference should move from room-sized GPU clusters to specialized chips.

Currently available as a chatbot demo and inference API service. HN #1 with 428 points and 278 comments.

**Source:** [taalas.com](https://taalas.com/the-path-to-ubiquitous-ai/) | [HN discussion](https://news.ycombinator.com/item?id=47086181)

---

## üß™ Research Frontier

### 1. Consistency Diffusion Language Models ‚Äî The End of Autoregressive Decoding?
**Together AI** | [Paper](https://www.together.ai/blog/consistency-diffusion-language-models) | Feb 19

The CDLM paper is the most practically significant research drop this cycle. The core insight: you can post-train existing LLMs into diffusion models that generate multiple tokens per step via iterative refinement, achieving **14.5x faster inference** with no quality loss. Block-wise causal masking preserves KV cache compatibility, making this deployable on existing infrastructure. If this approach scales to frontier models, it fundamentally changes the economics of agent loops ‚Äî tasks that currently take minutes could take seconds. Watch for adoption signals from inference providers.

### 2. Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents
**arxiv 2602.16699** | [Paper](https://arxiv.org/abs/2602.16699) | Feb 18-19

Directly relevant to anyone building agent workflows. This paper formalizes the problem of *when should an agent stop exploring and commit to an answer?* They introduce CTA ‚Äî a framework where you feed the LLM explicit cost-uncertainty priors so it can reason about whether to run one more test, fetch one more source, or commit. Tested on coding tasks and info-seeking QA. The key practical insight: **making cost-benefit tradeoffs explicit** (rather than implicit in prompts) helps agents discover more optimal strategies. If you're tuning agent prompts to balance thoroughness vs. speed, this framework gives you a principled approach.

### 3. Discovering Multiagent Learning Algorithms with LLMs
**arxiv 2602.16928** | [Paper](https://arxiv.org/abs/2602.16928) | Feb 18

Meta-research that uses Google's AlphaEvolve (an evolutionary coding agent) to **automatically discover new multi-agent learning algorithms**. Instead of manually designing how agents should coordinate, you let an LLM evolve the coordination algorithms themselves. The discovered algorithms outperform standard baselines in multi-agent settings. This is the "AI designing AI" pattern reaching multi-agent systems ‚Äî relevant context as Stripe and others scale autonomous agent fleets.

### 4. Retrieval Collapses When AI Pollutes the Web
**ACM Web Conference 2026** | [Paper](https://arxiv.org/html/2602.16136) | Feb 17

Timely research showing that as AI-generated content floods the web, RAG (retrieval-augmented generation) pipelines silently shift toward retrieving synthetic evidence ‚Äî creating a **self-reinforcing cycle of quality decline**. LLM-based rankers showed stronger suppression of synthetic content than traditional rankers, but the fundamental problem remains: the web's knowledge commons is degrading. For builders: if your agents rely on web search for research tasks, your retrieval quality is eroding in real-time. Consider curating trusted source lists rather than relying on open web search.

### 5. Isomorphic Labs Drug Design Engine (IsoDDE) ‚Äî "AlphaFold 4"
**Nature coverage** | [Article](https://www.nature.com/articles/d41586-026-00365-7) | Feb 19

Isomorphic Labs (DeepMind spin-off) released a 27-page technical report on IsoDDE, which **doubles AlphaFold 3's accuracy** on protein-ligand structure prediction and predicts binding affinities better than physics-based methods at a fraction of the compute. Nature calls it "an AlphaFold 4." The practical alpha: AI-driven drug discovery is moving from structure prediction to full design ‚Äî the pipeline from target to molecule candidate is getting automated. Meanwhile, Insilico Medicine + Eli Lilly published "From Prompt to Drug" in ACS Central Science (Feb 20), outlining a vision for fully autonomous prompt-to-drug pharmaceutical R&D.

**Source:** [Nature](https://www.nature.com/articles/d41586-026-00365-7) | [ACS Central Science](https://pubs.acs.org/doi/10.1021/acscentsci.5c01473)

---

## üé¨ Video & Image AI

### Disney Strikes Sora Deal While Suing ByteDance Over Seedance 2.0
**Score: 7.0/10** | CONFIRMED | The double standard crystallizes

CNN reports Disney struck a deal with OpenAI to give Sora access to trademarked characters (Mickey, Minnie) ‚Äî while simultaneously sending ByteDance cease-and-desist letters over Seedance 2.0 for "illegally using IP to train" the model. This is the clearest articulation of the emerging framework: studios want to *license* their IP to AI video tools they control and *litigate* against those they don't.

The Seedance copyright pile-on continues (all 5 major studios now), but the Disney-Sora deal reveals the endgame isn't banning AI video ‚Äî it's controlling the revenue flow.

**Source:** [CNN](https://www.cnn.com/2026/02/20/china/china-ai-seedance-intl-hnk-dst) | [LA Times](https://www.latimes.com/entertainment-arts/business/story/2026-02-18/mid-ai-scandal-hollywood-studios-threaten-bytedance-with-legal-action)

---

## ü§ñ New Models & Benchmarks

### Gemini 3.1 Pro (Feb 19, 2026) ‚Äî VERIFIED ‚úÖ
Google's new frontier model. Available in preview across Gemini API, Gemini CLI, Google Antigravity, Vertex AI, NotebookLM, GitHub Copilot. Pricing: $2/$12 per 1M tokens. 1M context window.

**Benchmark highlights:**
- ARC-AGI-2: 77.1% (2x+ vs Gemini 3 Pro)
- HLE: 44.4% (#1)
- GPQA Diamond: 94.3%
- SWE-Bench Verified: 80.6% (#1 agentic coding)
- Terminal-Bench 2.0: 68.5% (#1)
- LiveCodeBench Pro Elo: 2887

**Note:** Trails Claude Opus 4.6 in some specialized tasks per third-party reports, but leads in aggregate benchmarks and reasoning.

### GitHub Copilot Model Deprecations (Feb 17, effective Feb 19)
| Deprecated Model | Suggested Alternative |
|---|---|
| Claude Opus 4.1 | Claude Opus 4.6 |
| GPT-5 | GPT-5.2 |
| GPT-5-Codex | GPT-5.2-Codex |

---

## üéôÔ∏è Podcasts Worth Your Time

### 1. Latent Space ‚Äî AINews: Anthropic's Agent Autonomy Study
swyx and team break down Anthropic's "Measuring Agent Autonomy" paper with context on METR benchmarks, the autonomy trend curve, and what it means for the coding agent space. Includes data on World Labs and Ineffable Intelligence raising $1B rounds.

**Read/Listen:** [latent.space](https://www.latent.space/p/ainews-anthropics-agent-autonomy)

### 2. Latent Space ‚Äî Owning the AI Pareto Frontier (Jeff Dean)
Jeff Dean (Google) on the Pareto frontier of AI model capabilities, infrastructure decisions, and where Google sees the competitive landscape heading. Recorded recently, high signal for understanding the Gemini 3.1 strategy.

**Listen:** [radio.net/Latent Space](https://www.radio.net/podcast/latent-space-podcast) (Feb 12 episode, 1h23m)

### 3. The Neuron Daily ‚Äî Gemini CLI Creator Reveals All
Google Principal Engineer Taylor Mullen's first in-depth interview about Gemini CLI ‚Äî managing swarms of parallel AI agents, why the CLI is having a renaissance, and how Google ships 150 features/week with AI.

**Read:** [theneurondaily.com](https://www.theneurondaily.com/p/watch-google-s-team-ships-150-features-a-week-here-s-their-exact-playbook)

---

## üì∫ YouTube Picks

### 1. "Inside Claude Code With Its Creator Boris Cherny" ‚Äî Y Combinator Lightcone
Boris Cherny reveals the full Claude Code origin story, architecture decisions, and where it's heading. He predicts "software engineering" as a title starts to "go away" in 2026. Essential context for anyone using Claude Code daily.

**Watch:** [youtube.com/watch?v=PQU9o_5rHC4](https://www.youtube.com/watch?v=PQU9o_5rHC4)

### 2. "I Ranked Every AI Coding Assistant" ‚Äî Full Comparison
Comprehensive hands-on ranking of Cursor, Windsurf, Claude Code, Codex, Antigravity, OpenCode, Warp, and Gemini CLI. The kind of practitioner comparison that's actually useful.

**Watch:** [youtube.com/watch?v=NAWcnIebQ-o](https://www.youtube.com/watch?v=NAWcnIebQ-o)

### 3. "Claude Just Released Something To Fix AI Coding" ‚Äî Claude Skills Tutorial
Practical tutorial on building Claude Skills through real iteration and testing. Shows the actual workflow, not just the marketing pitch.

**Watch:** [youtube.com/watch?v=aEqKWI-0N0c](https://www.youtube.com/watch?v=aEqKWI-0N0c)

---

## üßë‚Äçüíª Coding Tips

### Try Gemini 3.1 Pro in Your Stack Today
The benchmarks are impressive but the real test is your workflow. Three ways to try it right now:

```bash
# 1. Gemini CLI (if you have it)
gemini --model gemini-3.1-pro-preview

# 2. Google AI Studio (browser)
# ‚Üí aistudio.google.com ‚Üí select gemini-3.1-pro-preview

# 3. GitHub Copilot (if you have Pro/Pro+/Business/Enterprise)
# ‚Üí Model picker ‚Üí Gemini 3.1 Pro
# (Enterprise admins: enable in Copilot settings first)
```

At $2/$12 per 1M tokens with 1M context, this is worth evaluating as a cost-effective alternative for tasks where you're currently spending on Opus or GPT-5.x.

### Learn a Codebase by Building a Visualizer
Jimmy Miller's post on HN (116 points) walks through learning Next.js's turbopack codebase by making random edits, breaking things, and building visualizations. The technique ‚Äî goal-setting ‚Üí random edits ‚Üí fix broken things ‚Üí read to answer questions ‚Üí build a visualizer ‚Äî is an underrated way to onboard onto any large codebase. Works especially well combined with AI agents: have Claude Code build the visualizer while you direct the exploration.

**Read:** [jimmyhmiller.com/learn-codebase-visualizer](https://jimmyhmiller.com/learn-codebase-visualizer)

---

## ‚öôÔ∏è Workflow Upgrades

### Copilot Coding Agent Model Picker Now Open to Teams
If your org uses GitHub Copilot Business or Enterprise, you can now select specific models for the coding agent (the async background agent that opens PRs). Previously this was Pro/Pro+ only. Available models include the full Claude and GPT-Codex lineup. This means you can point specific types of tasks at specific models ‚Äî use Opus 4.6 for complex refactoring, Sonnet 4.6 for quick fixes.

### Zed + Copilot: A Lightweight Rust-Based IDE Alternative
If you've been Copilot-curious about Zed but hesitated because of integration gaps, that barrier is gone. GitHub now officially supports Copilot in Zed (GA as of Feb 19). Zed ‚Äî the Rust-based editor from the Atom/Tree-sitter creators ‚Äî is exceptionally fast. Worth trying for projects where VS Code feels heavy.

### Monitor Your Agent Autonomy Patterns
Anthropic's study shows experienced users trend toward 40%+ auto-approve. If you're still manually approving every action in Claude Code, you might be the bottleneck. Consider using `--auto-approve` for well-understood tasks in repos with good test coverage, and reserve manual review for new or risky operations.

---

## üéØ Action Pack (Top 5 Experiments for Today)

1. **Try Gemini 3.1 Pro in AI Studio** ‚Äî The SWE-Bench and ARC-AGI-2 numbers are impressive. Test it on a real task you'd normally throw at Opus or Codex and compare quality + cost.
2. **Read Stripe's Minions Part 2** ‚Äî Study their autonomous agent architecture. Their one-shot approach may inspire changes to your own coding agent workflows.
3. **Check your GitHub Copilot model settings** ‚Äî If you're on Business/Enterprise, the model picker is now available for the coding agent. Try assigning Gemini 3.1 Pro to a GitHub issue.
4. **Read the CTA paper (arxiv 2602.16699)** ‚Äî If you're building agent workflows, the Calibrate-Then-Act framework for explicit cost-uncertainty tradeoffs is immediately applicable.
5. **Read the Together AI CDLM paper** ‚Äî 14x inference speedup is the kind of infrastructure shift that changes what's possible for agent loops. Understand the approach even if you're not deploying it yet.

---

## üìä Confidence & Sources

| Signal | Freshness | Confidence | Source Type |
|--------|-----------|------------|-------------|
| ggml.ai joins Hugging Face | Feb 20 | ‚úÖ CONFIRMED | GitHub Discussion, HN #1 |
| Gemini 3.1 Pro release | Feb 19 | ‚úÖ CONFIRMED | blog.google, Google Cloud blog |
| Gemini 3.1 Pro in Copilot | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Copilot model picker (Biz/Enterprise) | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Copilot in Zed GA | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Copilot model deprecations | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Stripe Minions Part 2 | Feb 19 | ‚úÖ CONFIRMED | stripe.dev |
| Anthropic agent autonomy study | Feb 19 | ‚úÖ CONFIRMED | anthropic.com/research |
| Together AI CDLM | Feb 19-20 | ‚úÖ CONFIRMED | together.ai blog |
| Taalas silicon Llama | Feb 19-20 | ‚úÖ CONFIRMED | taalas.com, HN |
| Disney-Sora deal | Feb 20 | ‚úÖ CONFIRMED | CNN |
| CTA paper | Feb 18-19 | ‚úÖ CONFIRMED | arxiv |
| Multi-agent AlphaEvolve paper | Feb 18 | ‚úÖ CONFIRMED | arxiv |
| Retrieval Collapses paper | Feb 17 | ‚úÖ CONFIRMED | ACM Web Conf 2026 |
| IsoDDE / Nature coverage | Feb 19 | ‚úÖ CONFIRMED | nature.com |

---

## üö´ Cut List

| Signal | Reason |
|--------|--------|
| Claude Max OAuth lockdown | Yesterday's #1 lead item ‚Äî no material update |
| Pentagon vs. Anthropic | Yesterday's #2 ‚Äî continuing coverage but no new facts |
| Sony/Seedance 2.0 cease-and-desist | Yesterday's #3 ‚Äî Disney-Sora angle is new (included) |
| OpenAI EVMbench | Yesterday's #3 ‚Äî no update |
| Opus 4.6 in Copilot | Yesterday's #4 ‚Äî new angles covered (model picker, deprecations, Zed) |
| Qodo 2.1 | Yesterday's #5 ‚Äî no update |
| Gen + Vercel safety | Yesterday's #6 ‚Äî no update |
| India AI Summit (Altman/Amodei awkward moment) | Viral but zero technical signal |
| Anthropic PAC $450K congressional race | Political, low builder relevance |
| OpenAI >$100B funding round / Nvidia $30B | Important but investment news, not builder-actionable |
| Trump tariffs struck down by Supreme Court | Major news, not AI-specific |
| Dario Amodei "trillionaires" warning | Interesting quote, no actionable signal |
