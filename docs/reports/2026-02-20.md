# AI Alpha Report ‚Äî February 20, 2026

*Daily intelligence for technical founders who ship*

---

## TL;DR

- üö® **Google drops Gemini 3.1 Pro** ‚Äî 77.1% ARC-AGI-2 (2x+ over 3 Pro), 80.6% SWE-Bench, beats Opus 4.6 and GPT-5.2 on most benchmarks. Available NOW in Gemini API, CLI, Antigravity, NotebookLM, and GitHub Copilot. $2/$12 per 1M tokens ‚Äî 7.5x cheaper than Opus on input.
- üèóÔ∏è **Stripe publishes Minions Part 2** ‚Äî 1,300+ PRs merged per week from fully autonomous coding agents. Zero human in the loop until review. Detailed architecture breakdown.
- ‚ö° **Together AI: Consistency Diffusion LMs** ‚Äî Up to 14x faster inference, no quality loss. Potential paradigm shift from autoregressive decoding. HN #5 with 139 points.
- üî¨ **Anthropic publishes agent autonomy study** ‚Äî Claude Code autonomous sessions nearly doubled (25‚Üí45+ min) in 3 months. 40%+ experienced users run full auto-approve.
- üßÆ **Taalas unveils silicon Llama 3.1 8B** ‚Äî 17K tokens/sec per user via custom silicon. 10x faster than current SotA. HN #1.

---

## üß≠ Narrative Radar

### What's Shifting This Week

**1. The Benchmark Race Just Flipped ‚Äî Google Retakes the Crown**
Gemini 3.1 Pro landing with 77.1% on ARC-AGI-2 (more than double Gemini 3 Pro), 44.4% on Humanity's Last Exam (vs Opus 4.6's 40%, GPT-5.2's 34.5%), and 80.6% SWE-Bench Verified isn't just another model bump ‚Äî it's Google genuinely competing at the frontier again. The pricing ($2/$12 per 1M tokens, 7.5x cheaper than Opus on input) makes this especially disruptive. With Gemini CLI, Antigravity IDE, and now GitHub Copilot integration, Google is competing across the entire developer surface for the first time. The question: does benchmark lead translate to practitioner adoption?

**2. Coding Agents Are Graduating From "Experiment" to "Infrastructure"**
Stripe publishing detailed numbers (1,300+ PRs/week from autonomous agents, zero human until review) is the clearest signal yet that coding agents aren't a novelty ‚Äî they're production infrastructure at top-tier engineering orgs. Simultaneously, Anthropic publishing data showing Claude Code sessions nearly doubling in autonomous runtime and 40%+ of experienced users running full auto-approve means the human-in-the-loop is shrinking fast. The agent autonomy curve is steepening.

**3. The Inference Cost Floor Is Collapsing**
Three separate signals converging: Together AI's consistency diffusion models (14x faster inference), Taalas shipping custom silicon at 17K tokens/sec (10x current SotA), and Google pricing Gemini 3.1 Pro at 7.5x cheaper than Opus. The infrastructure layer is saying: the marginal cost of intelligence is heading toward zero, and it's happening faster than most people expect. This directly enables the autonomous agent future Stripe and Anthropic are describing.

### What's Fading
- The "Anthropic OAuth lockdown" panic from this week is settling ‚Äî people have switched or adapted
- India AI Summit generated lots of announcements but no breakthrough products
- Pentagon vs. Anthropic is becoming background noise without new material developments

---

## üöÄ Top Early Moves

### 1. Google Releases Gemini 3.1 Pro ‚Äî Benchmark Leader at Fraction of the Cost üî•üî•üî•
**Score: 9.5/10** | CONFIRMED | Major new model, test it today

Google dropped Gemini 3.1 Pro on Feb 19, building on the "upgraded core intelligence" that powered last week's Gemini 3 Deep Think update. The benchmarks are genuinely impressive:

| Benchmark | Gemini 3.1 Pro | Claude Opus 4.6 | GPT-5.2 |
|-----------|---------------|-----------------|---------|
| ARC-AGI-2 | **77.1%** | ‚Äî | ‚Äî |
| HLE | **44.4%** | 40.0% | 34.5% |
| GPQA Diamond | **94.3%** | ‚Äî | ‚Äî |
| SWE-Bench Verified | **80.6%** | ‚Äî | ‚Äî |
| LiveCodeBench Pro Elo | **2887** | ‚Äî | ‚Äî |
| Terminal-Bench 2.0 | **68.5%** | ‚Äî | ‚Äî |

**Pricing:** $2 input / $12 output per 1M tokens ‚Äî **7.5x cheaper than Opus 4.6 on input**. 1M token context window.

**Where to try it:**
- [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-3.1-pro-preview) (preview)
- [Gemini CLI](https://geminicli.com/)
- [Google Antigravity IDE](https://antigravity.google/)
- Vertex AI, NotebookLM (rolling out)
- GitHub Copilot (public preview, see below)

**JetBrains quote:** "Up to 15% improvement over best Gemini 3 Pro Preview runs. Stronger, faster, more efficient, requiring fewer output tokens."

**Source:** [blog.google](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/) | [Google Cloud blog](https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-1-pro-on-gemini-cli-gemini-enterprise-and-vertex-ai) | [Ars Technica](https://arstechnica.com/google/2026/02/google-announces-gemini-3-1-pro-says-its-better-at-complex-problem-solving/) | [VentureBeat](https://venturebeat.com/technology/google-launches-gemini-3-1-pro-retaking-ai-crown-with-2x-reasoning)

---

### 2. Stripe "Minions" Part 2 ‚Äî 1,300+ PRs/Week From Autonomous Agents üèóÔ∏è
**Score: 9.0/10** | CONFIRMED | The best public data on coding agents at scale

Stripe published Part 2 of their Minions blog series. Minions are Stripe's fully unattended coding agents ‚Äî given a task, they write code, run tests, fix issues, and submit a pull request with zero human in the loop until review. Key numbers:

- **1,300+ PRs merged per week** from Minions alone
- Tasks range from fixing flaky tests to implementing features
- "One-shot, end-to-end" ‚Äî they don't iterate with a human mid-task

This is the most detailed public account of autonomous coding agents operating at scale in a top-tier engineering org. Required reading for anyone building or evaluating coding agent workflows.

**Source:** [stripe.dev/blog](https://stripe.dev/blog/minions-stripes-one-shot-end-to-end-coding-agents-part-2) | [Analytics India Magazine](https://analyticsindiamag.com/ai-news/stripes-autonomous-coding-agents-generate-over-1300-prs-a-week)

---

### 3. GitHub Copilot: Gemini 3.1 Pro + Model Picker for Enterprise + Model Purge
**Score: 8.5/10** | CONFIRMED | Three changelog updates, all actionable

Three GitHub Changelog entries dropped Feb 19:

**a) Gemini 3.1 Pro in Copilot (public preview)**
Available in VS Code (chat, ask, edit, agent), Visual Studio, github.com, GitHub Mobile. Pro, Pro+, Business, Enterprise. Excels at "edit-then-test loops with high tool precision."

**b) Model picker for Copilot coding agent ‚Üí Business/Enterprise**
Previously Pro/Pro+ only. Now Business and Enterprise users can select from: Claude Opus 4.5/4.6, Sonnet 4.5/4.6, GPT-5.1-Codex-Max, GPT-5.2-Codex, GPT-5.3-Codex. Default (Auto): Claude Sonnet 4.6.

**c) Deprecated models: Claude Opus 4.1, GPT-5, GPT-5-Codex**
All deprecated Feb 17 across all Copilot experiences. Suggested alternatives: Opus 4.6, GPT-5.2, GPT-5.2-Codex. Enterprise admins must enable replacement models in policy settings.

**Source:** [Gemini 3.1 Pro in Copilot](https://github.blog/changelog/2026-02-19-gemini-3-1-pro-is-now-in-public-preview-in-github-copilot/) | [Model picker](https://github.blog/changelog/2026-02-19-model-picker-for-copilot-coding-agent-for-copilot-business-and-enterprise-users/) | [Deprecations](https://github.blog/changelog/2026-02-19-selected-anthropic-and-openai-models-are-now-deprecated/)

---

### 4. Anthropic Publishes Agent Autonomy Study üî¨
**Score: 8.0/10** | CONFIRMED | Hard data on how coding agents are actually used

Anthropic published "Measuring AI agent autonomy in practice" ‚Äî analyzing millions of Claude Code interactions using their privacy-preserving tool Clio. Key findings:

- **Autonomous session length nearly doubled** from <25 min to >45 min in 3 months (smooth across model releases, suggesting users are granting more autonomy, not just capability gains)
- **40%+ experienced users use full auto-approve** (vs ~20% new users)
- **Claude Code pauses for clarification more often than humans interrupt it** ‚Äî agent-initiated stops > human interrupts on complex tasks
- **Most usage is coding**, but data shows the next agent targets: research, writing, data analysis
- METR's 50th-percentile metric shows agents now doing ~5 hours of human-equivalent work

This is the first large-scale empirical study of real-world agent autonomy patterns. The trend line is clear: autonomy is expanding rapidly.

**Source:** [anthropic.com/research/measuring-agent-autonomy](https://www.anthropic.com/research/measuring-agent-autonomy) | [Latent Space analysis](https://www.latent.space/p/ainews-anthropics-agent-autonomy)

---

### 5. Together AI: Consistency Diffusion Language Models ‚Äî 14x Faster Inference ‚ö°
**Score: 8.0/10** | CONFIRMED | Potential paradigm shift in LLM inference

Together AI published a paper on Consistency Diffusion Language Models (CDLM) ‚Äî a post-training recipe that makes diffusion-based language models viable for production inference. Instead of autoregressive token-by-token generation, CDLMs iteratively refine masked sequences, generating multiple tokens per step.

**Key results:** Up to 14x faster inference vs autoregressive models with no quality degradation. Uses block-wise causal masking for exact KV caching compatibility (solving the main bottleneck of diffusion LMs). The approach works as a post-training step on existing models.

**Why this matters:** If diffusion LMs become practical, the entire inference stack changes. 14x throughput improvement means agent loops that take minutes could take seconds. HN #5 with 139 points and 47 comments ‚Äî the technical community is paying attention.

**Source:** [together.ai/blog](https://www.together.ai/blog/consistency-diffusion-language-models) | [HN discussion](https://news.ycombinator.com/item?id=47083648)

---

### 6. Taalas: Custom Silicon Llama at 17K Tokens/Sec üßÆ
**Score: 7.5/10** | CONFIRMED | Hardware inference breakthrough, HN #1

Taalas unveiled their first product: a hard-wired Llama 3.1 8B implemented in custom silicon, achieving 17K tokens/sec per user ‚Äî nearly 10x faster than current state of the art. The startup converts any AI model into custom silicon in ~2 months.

**The thesis:** Instead of scaling data centers, specialize hardware for individual models. Like how ENIAC ‚Üí transistors ‚Üí smartphones happened for computing, AI inference should move from room-sized GPU clusters to specialized chips.

Currently available as a chatbot demo and inference API service. HN #1 with 124 points and 87 comments (trending fast).

**Source:** [taalas.com](https://taalas.com/the-path-to-ubiquitous-ai/) | [HN discussion](https://news.ycombinator.com/item?id=47086181)

---

## üé¨ Video & Image AI

### Disney Strikes Sora Deal While Suing ByteDance Over Seedance 2.0
**Score: 7.0/10** | CONFIRMED | The double standard crystallizes

CNN reports Disney struck a deal with OpenAI to give Sora access to trademarked characters (Mickey, Minnie) ‚Äî while simultaneously sending ByteDance cease-and-desist letters over Seedance 2.0 for "illegally using IP to train" the model. This is the clearest articulation of the emerging framework: studios want to *license* their IP to AI video tools they control and *litigate* against those they don't.

The Seedance copyright pile-on continues (all 5 major studios now), but the Disney-Sora deal reveals the endgame isn't banning AI video ‚Äî it's controlling the revenue flow.

**Source:** [CNN](https://www.cnn.com/2026/02/20/china/china-ai-seedance-intl-hnk-dst) | [LA Times](https://www.latimes.com/entertainment-arts/business/story/2026-02-18/mid-ai-scandal-hollywood-studios-threaten-bytedance-with-legal-action)

---

## ü§ñ New Models & Benchmarks

### Gemini 3.1 Pro (Feb 19, 2026) ‚Äî VERIFIED ‚úÖ
Google's new frontier model. Available in preview across Gemini API, Gemini CLI, Google Antigravity, Vertex AI, NotebookLM, GitHub Copilot. Pricing: $2/$12 per 1M tokens. 1M context window.

**Benchmark highlights:**
- ARC-AGI-2: 77.1% (2x+ vs Gemini 3 Pro)
- HLE: 44.4% (#1)
- GPQA Diamond: 94.3%
- SWE-Bench Verified: 80.6% (#1 agentic coding)
- Terminal-Bench 2.0: 68.5% (#1)
- LiveCodeBench Pro Elo: 2887

**Note:** Trails Claude Opus 4.6 in some specialized tasks per third-party reports, but leads in aggregate benchmarks and reasoning.

### GitHub Copilot Model Deprecations (Feb 17, effective Feb 19)
| Deprecated Model | Suggested Alternative |
|---|---|
| Claude Opus 4.1 | Claude Opus 4.6 |
| GPT-5 | GPT-5.2 |
| GPT-5-Codex | GPT-5.2-Codex |

---

## üéôÔ∏è Podcasts Worth Your Time

### 1. Latent Space ‚Äî AINews: Anthropic's Agent Autonomy Study
swyx and team break down Anthropic's "Measuring Agent Autonomy" paper with context on METR benchmarks, the autonomy trend curve, and what it means for the coding agent space. Includes data on World Labs and Ineffable Intelligence raising $1B rounds.

**Read/Listen:** [latent.space](https://www.latent.space/p/ainews-anthropics-agent-autonomy)

### 2. Latent Space ‚Äî Owning the AI Pareto Frontier (Jeff Dean)
Jeff Dean (Google) on the Pareto frontier of AI model capabilities, infrastructure decisions, and where Google sees the competitive landscape heading. Recorded recently, high signal for understanding the Gemini 3.1 strategy.

**Listen:** [radio.net/Latent Space](https://www.radio.net/podcast/latent-space-podcast) (Feb 12 episode, 1h23m)

### 3. The Neuron Daily ‚Äî Gemini CLI Creator Reveals All
Google Principal Engineer Taylor Mullen's first in-depth interview about Gemini CLI ‚Äî managing swarms of parallel AI agents, why the CLI is having a renaissance, and how Google ships 150 features/week with AI.

**Read:** [theneurondaily.com](https://www.theneurondaily.com/p/watch-google-s-team-ships-150-features-a-week-here-s-their-exact-playbook)

---

## üì∫ YouTube Picks

### 1. "Inside Claude Code With Its Creator Boris Cherny" ‚Äî Y Combinator Lightcone
Boris Cherny reveals the full Claude Code origin story, architecture decisions, and where it's heading. He predicts "software engineering" as a title starts to "go away" in 2026. Essential context for anyone using Claude Code daily.

**Watch:** [youtube.com/watch?v=PQU9o_5rHC4](https://www.youtube.com/watch?v=PQU9o_5rHC4)

### 2. "I Ranked Every AI Coding Assistant" ‚Äî Full Comparison
Comprehensive hands-on ranking of Cursor, Windsurf, Claude Code, Codex, Antigravity, OpenCode, Warp, and Gemini CLI. The kind of practitioner comparison that's actually useful.

**Watch:** [youtube.com/watch?v=NAWcnIebQ-o](https://www.youtube.com/watch?v=NAWcnIebQ-o)

### 3. "Claude Just Released Something To Fix AI Coding" ‚Äî Claude Skills Tutorial
Practical tutorial on building Claude Skills through real iteration and testing. Shows the actual workflow, not just the marketing pitch.

**Watch:** [youtube.com/watch?v=aEqKWI-0N0c](https://www.youtube.com/watch?v=aEqKWI-0N0c)

---

## üßë‚Äçüíª Coding Tips

### Try Gemini 3.1 Pro in Your Stack Today
The benchmarks are impressive but the real test is your workflow. Three ways to try it right now:

```bash
# 1. Gemini CLI (if you have it)
gemini --model gemini-3.1-pro-preview

# 2. Google AI Studio (browser)
# ‚Üí aistudio.google.com ‚Üí select gemini-3.1-pro-preview

# 3. GitHub Copilot (if you have Pro/Pro+/Business/Enterprise)
# ‚Üí Model picker ‚Üí Gemini 3.1 Pro
# (Enterprise admins: enable in Copilot settings first)
```

At $2/$12 per 1M tokens with 1M context, this is worth evaluating as a cost-effective alternative for tasks where you're currently spending on Opus or GPT-5.x.

### Read Stripe's Minions Architecture
Even if you can't replicate Stripe's scale, their approach to autonomous agent design is instructive: one-shot task assignment, no human-in-the-loop until PR review, comprehensive test-fix loops. The architectural pattern translates to any team building agent workflows.

---

## ‚öôÔ∏è Workflow Upgrades

### Copilot Coding Agent Model Picker Now Open to Teams
If your org uses GitHub Copilot Business or Enterprise, you can now select specific models for the coding agent (the async background agent that opens PRs). Previously this was Pro/Pro+ only. Available models include the full Claude and GPT-Codex lineup. This means you can point specific types of tasks at specific models ‚Äî use Opus 4.6 for complex refactoring, Sonnet 4.6 for quick fixes.

### Monitor Your Agent Autonomy Patterns
Anthropic's study shows experienced users trend toward 40%+ auto-approve. If you're still manually approving every action in Claude Code, you might be the bottleneck. Consider using `--auto-approve` for well-understood tasks in repos with good test coverage, and reserve manual review for new or risky operations.

---

## üéØ Action Pack (Top 5 Experiments for Today)

1. **Try Gemini 3.1 Pro in AI Studio** ‚Äî The SWE-Bench and ARC-AGI-2 numbers are impressive. Test it on a real task you'd normally throw at Opus or Codex and compare quality + cost.
2. **Read Stripe's Minions Part 2** ‚Äî Study their autonomous agent architecture. Their one-shot approach may inspire changes to your own coding agent workflows.
3. **Check your GitHub Copilot model settings** ‚Äî If you're on Business/Enterprise, the model picker is now available for the coding agent. Try assigning Gemini 3.1 Pro to a GitHub issue.
4. **Evaluate your Claude Code auto-approve habits** ‚Äî Anthropic's data shows experienced users auto-approve 40%+ of sessions. If you're still manually confirming everything, you're leaving productivity on the table.
5. **Read the Together AI CDLM paper** ‚Äî 14x inference speedup is the kind of infrastructure shift that changes what's possible for agent loops. Understand the approach even if you're not deploying it yet.

---

## üìä Confidence & Sources

| Signal | Freshness | Confidence | Source Type |
|--------|-----------|------------|-------------|
| Gemini 3.1 Pro release | Feb 19 | ‚úÖ CONFIRMED | blog.google, Google Cloud blog |
| Gemini 3.1 Pro in Copilot | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Copilot model picker (Biz/Enterprise) | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Copilot model deprecations | Feb 19 | ‚úÖ CONFIRMED | GitHub Changelog |
| Stripe Minions Part 2 | Feb 19 | ‚úÖ CONFIRMED | stripe.dev |
| Anthropic agent autonomy study | Feb 19 | ‚úÖ CONFIRMED | anthropic.com/research |
| Together AI CDLM | Feb 19-20 | ‚úÖ CONFIRMED | together.ai blog |
| Taalas silicon Llama | Feb 19-20 | ‚úÖ CONFIRMED | taalas.com, HN #1 |
| Disney-Sora deal | Feb 20 | ‚úÖ CONFIRMED | CNN |
| Figma Q4 earnings (+15%) | Feb 18-19 | ‚úÖ CONFIRMED | Reuters, CNBC |
| Nvidia $30B ‚Üí OpenAI | Feb 19-20 | ‚úÖ CONFIRMED | Reuters, CNBC, FT |
| Boris Cherny Lightcone interview | Feb 17 | ‚úÖ CONFIRMED | YouTube |

---

## üö´ Cut List

| Signal | Reason |
|--------|--------|
| Claude Max OAuth lockdown | Yesterday's #1 lead item ‚Äî no material update |
| Pentagon vs. Anthropic | Yesterday's #2 ‚Äî continuing coverage but no new facts |
| Sony/Seedance 2.0 cease-and-desist | Yesterday's #3 ‚Äî Disney-Sora angle is new (included) |
| OpenAI EVMbench | Yesterday's #3 ‚Äî no update |
| Opus 4.6 in Copilot | Yesterday's #4 ‚Äî new angles covered (model picker, deprecations) |
| Qodo 2.1 | Yesterday's #5 ‚Äî no update |
| Gen + Vercel safety | Yesterday's #6 ‚Äî no update |
| India AI Summit (Altman/Amodei awkward moment) | Viral but zero technical signal |
| Anthropic PAC $450K congressional race | Political, low builder relevance |
| OpenAI >$100B funding round / Nvidia $30B | Important but investment news, not builder-actionable |
| Figma Q4 earnings | Good signal for AI monetization thesis but not directly actionable |
| NYT "AI Disruption Has Arrived" opinion piece | Mainstream, Juan already knows this |
| Sarvam 30B/105B | Covered yesterday, India-focused |
| AI Coding tools "mixed blessing" for OSS (TechCrunch) | Interesting essay, no actionable signal |
| HN "AI is not a coworker, it's an exoskeleton" | Good essay, low novelty for advanced practitioners |
